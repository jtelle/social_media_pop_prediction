python 3.11


This project contains scripts for preparing data and training multiple machine learning models. It is organized into two main modules:

- **Data Preparation**: `data_preparation.py`
- **Model Training**: `model_training.py`

---

## Data Preparation Module

The `data_preparation.py` script handles the entire data preprocessing pipeline, transforming raw input data into a clean, structured format ready for machine learning.

### Key Functionalities

- **`load_data(file_path)`**  
  Loads the raw dataset from a specified CSV file into a Pandas DataFrame.

- **`transform_features(df)`**  
  Applies a `log1p` transformation to the `shares` and `n_comments` columns to normalize skewed distributions.

- **`handle_missing_values(df)`**  
  Fills missing numerical values (e.g., `num_hrefs`, `num_imgs`) with their median, and categorical values in `data_channel` with the mode.

- **`encode_categorical_features(df)`**  
  One-hot encodes the `weekday` and `data_channel` features using `drop_first=True` to avoid multicollinearity.

- **`split_data(df_processed)`**  
  Drops identifier columns (`ID`, `URL`), splits into features (`X`) and target (`y`), and applies an 80/20 train-test split with fixed `random_state`.

- **`save_prepared_data(X_train, X_test, y_train, y_test, output_dir)`**  
  Saves processed arrays as `.npy` files in the specified output directory.

- **`feature engineering`**
  Custom Feature Creation: A dedicated function (create_new_features) has been implemented to engineer new, potentially more informative features from the raw dataset. This includes creating ratios (e.g., avg_words_per_link, avg_words_per_video) and interaction terms (e.g., num_imgs_x_tokens) to enhance the model's ability to capture complex patterns. This step aims to improve predictive performance by providing richer input to the models.

- **`main()`**  
  Orchestrates the entire flow, including loading raw data, transforming it, and saving the results in a structured way.

### Usage

Ensure your raw CSV file (e.g., `data.csv`) is in the `../data` directory. Run the script using:

```bash
python src/data_preparation.py
```

## Model Training Module

The `model_training.py` script is responsible for training, evaluating, and saving multiple regression models using preprocessed datasets generated by `data_preparation.py`. It helps identify the most accurate model based on standard evaluation metrics.

### Key Functionalities

- **Data Loading**  
  Loads the prepared datasets (`X_train.npy`, `X_test.npy`, `y_train.npy`, `y_test.npy`) from the specified directory using NumPy.

- **Model Training**  
  Trains a suite of regression models:
  - `XGBoostRegressor`
  - `LinearRegression`
  - `RandomForestRegressor`
  - `GradientBoostingRegressor`

- **Performance Evaluation**  
  Evaluates each model on the test set using:
  - Mean Absolute Error (MAE)
  - Mean Squared Error (MSE)
  - Root Mean Squared Error (RMSE)
  - R-squared score (R²)

- **Model Comparison**  
  Outputs a performance summary table of all trained models, sorted by R² score.

- **Best Model Selection**  
  Identifies the best-performing model (highest R² score) and saves it using `joblib`.

- **Model Saving**  
  Stores the trained model as a `.joblib` file for future use, making it easy to deploy or fine-tune later.

- **Expanded Model Training with Hyperparameter Tuning**
   The pipeline supports training and hyperparameter tuning for multiple regression models, including Linear Regression, RandomForest Regressor, XGBoost Regressor, and Gradient Boosting Regressor.

- **Real-time Pipeline Output Streaming**
   The main.py orchestrator script has been improved to stream the output of individual pipeline stages (data_preparation.py, model_training.py) in real-time. This provides immediate feedback and eliminates the "stuck" feeling during long-running processes like hyperparameter tuning.

- **Enhanced Tuning Progress Visibility**
   The model_training.py script provides verbose and granular progress updates during hyperparameter tuning, making it easier to monitor the status of individual model fits.

** The random forest regressor has been commented out in the config file to save testing time, it can be uncommented if necessary. 

### Usage

Ensure the required libraries are installed:

```bash
pip install requirements.txt
```

### `tests/test_data_preparation.py`

This script contains the unit tests for the `data_preparation.py` module. Its purpose is to ensure that individual functions within the data preparation pipeline work correctly and as expected.

The tests cover key functionalities, including:
* Loading data from a CSV file.
* Applying log transformations to specified columns.
* Handling missing values (numerical imputation with median, categorical with mode).
* Performing one-hot encoding on categorical features.
* Splitting the dataset into training and testing sets.
* Saving the prepared data to `.npy` files.

Running these tests helps confirm the reliability and correctness of the data preprocessing steps.
